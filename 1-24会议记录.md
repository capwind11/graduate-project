针对SwissLog的日志解析部分的后两步有疑问。第三步是否与第一步有重合？第四步设立的意义何在？第三步主要是为了将日志中出现次数足够多的非valid words（如datanode的领域知识）提取成常量。而第四步的前缀树是为了解决模版变量中存在valid words的问题。

毕设工作讨论：
将interleaved日志的workflow提取视作单独任务是不科学的。如果出现一些异常的workflow（或者是以往没有出现过的workflow），那么这个workflow就不能被正确识别。我们应该利用工作流提取的原理去检测是否存在异常。

锦洋提出，由于log parser对后面的模型没有太大的帮助，我们不必使用log parser。模型可使用bert编码输入日志得到语义信息，将编码信息聚类，每个类有相似的语义，因此可视作一个模版。基于Feifei Li的工作原理，使用LSTM模型来进行半监督学习。由于在日志系统中样本极度不均衡，正样本的数量远大于负样本的数量，因此，我们可以直接将日志序列编码作为输入，一个log entry的语义编码视作一个time step，将真实的语义编码类别和预测的语义类别比较，求出loss，再根据这个loss进行多次训练迭代，最终得到工作流的概率分布，概率高的分支可视作正常工作流。这个模型就可以对快速迭代更新的日志系统鲁棒。整个想法比较连贯，动机也非常明显，但是缺少了性能相关的考量。bert编码开销是否太大？建议对log parser改进，得到一个online版的log parser，效果更好。

log parser的改进思路有：在log parser的部分，可以结合SwissLog和Drain的思想，优化step3和step4部分。step 3中的LCS时间复杂度很高，可优化。step4中的prefix tree建立后，可以比较子树的相似性，从而将相似的logs聚合在一起。